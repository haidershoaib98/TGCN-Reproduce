{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haidershoaib98/TGCN-Reproduce/blob/main/6322_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8gwrZ2vpanj"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rjYpXF9egsEy",
        "outputId": "3a6c13f3-bae5-45f0-e6c7-50ae115f51bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libspatialindex-c4v5 libspatialindex4v5\n",
            "The following NEW packages will be installed:\n",
            "  libspatialindex-c4v5 libspatialindex-dev libspatialindex4v5\n",
            "0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 555 kB of archives.\n",
            "After this operation, 3,308 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libspatialindex4v5 amd64 1.8.5-5 [219 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libspatialindex-c4v5 amd64 1.8.5-5 [51.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libspatialindex-dev amd64 1.8.5-5 [285 kB]\n",
            "Fetched 555 kB in 1s (470 kB/s)\n",
            "Selecting previously unselected package libspatialindex4v5:amd64.\n",
            "(Reading database ... 156210 files and directories currently installed.)\n",
            "Preparing to unpack .../libspatialindex4v5_1.8.5-5_amd64.deb ...\n",
            "Unpacking libspatialindex4v5:amd64 (1.8.5-5) ...\n",
            "Selecting previously unselected package libspatialindex-c4v5:amd64.\n",
            "Preparing to unpack .../libspatialindex-c4v5_1.8.5-5_amd64.deb ...\n",
            "Unpacking libspatialindex-c4v5:amd64 (1.8.5-5) ...\n",
            "Selecting previously unselected package libspatialindex-dev:amd64.\n",
            "Preparing to unpack .../libspatialindex-dev_1.8.5-5_amd64.deb ...\n",
            "Unpacking libspatialindex-dev:amd64 (1.8.5-5) ...\n",
            "Setting up libspatialindex4v5:amd64 (1.8.5-5) ...\n",
            "Setting up libspatialindex-c4v5:amd64 (1.8.5-5) ...\n",
            "Setting up libspatialindex-dev:amd64 (1.8.5-5) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Collecting osmnx\n",
            "  Downloading osmnx-1.1.2-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting pyproj>=3.2\n",
            "  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 17.2 MB/s \n",
            "\u001b[?25hCollecting Rtree>=0.9\n",
            "  Downloading Rtree-1.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.7/dist-packages (from osmnx) (2.6.3)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.3.5)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.21.5)\n",
            "Collecting matplotlib>=3.4\n",
            "  Downloading matplotlib-3.5.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 52.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Shapely<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.8.1.post1)\n",
            "Collecting geopandas>=0.10\n",
            "  Downloading geopandas-0.10.2-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 59.6 MB/s \n",
            "\u001b[?25hCollecting fiona>=1.8\n",
            "  Downloading Fiona-1.8.21-cp37-cp37m-manylinux2014_x86_64.whl (16.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.7 MB 523 kB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (2021.10.8)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (1.15.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (7.1.2)\n",
            "Collecting click-plugins>=1.0\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting cligj>=0.5\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (21.4.0)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.31.2-py3-none-any.whl (899 kB)\n",
            "\u001b[K     |████████████████████████████████| 899 kB 66.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (3.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (1.4.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.4->osmnx) (3.10.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3->osmnx) (2018.9)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (2.10)\n",
            "Installing collected packages: munch, cligj, click-plugins, pyproj, fonttools, fiona, Rtree, requests, matplotlib, geopandas, osmnx\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Rtree-1.0.0 click-plugins-1.1.1 cligj-0.7.2 fiona-1.8.21 fonttools-4.31.2 geopandas-0.10.2 matplotlib-3.5.1 munch-2.5.0 osmnx-1.1.2 pyproj-3.2.1 requests-2.27.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!apt install libspatialindex-dev\n",
        "!pip install osmnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaxuZeJqFe5j",
        "outputId": "e7ea0a17-1f8d-425a-d9bb-3a11d1f8e404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.7/dist-packages (3.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBGc1obVImt5",
        "outputId": "d32fecd9-5f7b-4c3c-f88b-57c4522e67a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-b4o_vb3g\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-b4o_vb3g\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (1.21.5)\n",
            "Collecting PyYAML>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (21.3)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (1.10.0+cu111)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.3-py3-none-any.whl (398 kB)\n",
            "\u001b[K     |████████████████████████████████| 398 kB 59.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (4.63.0)\n",
            "Collecting typing-extensions>=4.0.0\n",
            "  Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.7.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.44.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.37.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.35.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.3.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.4.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.2.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 74.2 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 74.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (21.4.0)\n",
            "Building wheels for collected packages: pytorch-lightning\n",
            "  Building wheel for pytorch-lightning (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorch-lightning: filename=pytorch_lightning-1.7.0.dev0-py3-none-any.whl size=580865 sha256=ce4b7b15a74ac01f6b30ae74a0a17ad3bce2f1f69cc708464d0708153d20811a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jtnn2e9_/wheels/18/24/dd/8e13b7dfcda990eb2b099a57bb6d705f6d43fb53ecb4ed07bf\n",
            "Successfully built pytorch-lightning\n",
            "Installing collected packages: typing-extensions, multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, PyYAML, pytorch-lightning\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "arviz 0.11.4 requires typing-extensions<4,>=3.7.4.3, but you have typing-extensions 4.1.1 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.7.0.dev0 torchmetrics-0.7.3 typing-extensions-4.1.1 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvqKfpaqg-bo"
      },
      "outputs": [],
      "source": [
        "# Import Python libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as sp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "import torch.optim\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from torch.utils.data.dataloader import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajX-IF8tphcg"
      },
      "source": [
        "# Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvV3bIfdhXCj",
        "outputId": "167b91ff-9bf3-4ee0-e99b-5012278e0741"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sArL9QxkAHID",
        "outputId": "16816f95-f3c2-4a98-bcd2-9d10b936749b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TGCN-Reproduce'...\n",
            "remote: Enumerating objects: 39, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 39 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (39/39), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/haidershoaib98/TGCN-Reproduce.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV12s3u31kT4"
      },
      "source": [
        "Getting data from github clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Khm-XHs1oBv"
      },
      "outputs": [],
      "source": [
        "# Los Loop\n",
        "f = pd.read_csv('TGCN-Reproduce/data/los_speed.csv')\n",
        "f_adj = pd.read_csv('TGCN-Reproduce/data/los_adj.csv')\n",
        "data = np.array(f, dtype=np.float32)\n",
        "adj = np.array(f_adj, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOtiZfFouZLy"
      },
      "source": [
        "This cell creates the dataset into tensors (Taken from T-GCN source code since it is a very specific way to seperate the training and testing sets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huKQ-A5NRUmU"
      },
      "outputs": [],
      "source": [
        "def generate_dataset(\n",
        "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param data: feature matrix\n",
        "    :param seq_len: length of the train data sequence\n",
        "    :param pre_len: length of the prediction data sequence\n",
        "    :param time_len: length of the time series in total\n",
        "    :param split_ratio: proportion of the training set\n",
        "    :param normalize: scale the data to (0, 1], divide by the maximum value in the data\n",
        "    :return: train set (X, Y) and test set (X, Y)\n",
        "    \"\"\"\n",
        "    if time_len is None:\n",
        "        time_len = data.shape[0]\n",
        "    if normalize:\n",
        "        max_val = np.max(data)\n",
        "        data = data / max_val\n",
        "    train_size = int(time_len * split_ratio)\n",
        "    train_data = data[:train_size]\n",
        "    test_data = data[train_size:time_len]\n",
        "    train_X, train_Y, test_X, test_Y = list(), list(), list(), list()\n",
        "    for i in range(len(train_data) - seq_len - pre_len):\n",
        "        train_X.append(np.array(train_data[i : i + seq_len]))\n",
        "        train_Y.append(np.array(train_data[i + seq_len : i + seq_len + pre_len]))\n",
        "    for i in range(len(test_data) - seq_len - pre_len):\n",
        "        test_X.append(np.array(test_data[i : i + seq_len]))\n",
        "        test_Y.append(np.array(test_data[i + seq_len : i + seq_len + pre_len]))\n",
        "    return np.array(train_X), np.array(train_Y), np.array(test_X), np.array(test_Y)\n",
        "\n",
        "\n",
        "def generate_torch_datasets(\n",
        "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
        "):\n",
        "    train_X, train_Y, test_X, test_Y = generate_dataset(\n",
        "        data,\n",
        "        seq_len,\n",
        "        pre_len,\n",
        "        time_len=time_len,\n",
        "        split_ratio=split_ratio,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(train_X), torch.FloatTensor(train_Y)\n",
        "    )\n",
        "    test_dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(test_X), torch.FloatTensor(test_Y)\n",
        "    )\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate train and test data"
      ],
      "metadata": {
        "id": "RLDtfaNUJZ47"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27neyI6OVBam"
      },
      "outputs": [],
      "source": [
        "tr_x, tr_y, tx, ty = generate_dataset(data, 12, 3)\n",
        "train, test = generate_torch_datasets(data, 12, 64)\n",
        "train_data = DataLoader(train, batch_size=64)\n",
        "test_data = DataLoader(test, batch_size=len(test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikIHjZsBpoqR"
      },
      "source": [
        "# T-GCN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_YNUSSiukR8"
      },
      "source": [
        "Laplacian of adjacency matrix A (Specific code taken from source code of T-GCN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQR35-EiF8xg"
      },
      "outputs": [],
      "source": [
        "def calculate_laplacian_with_self_loop(matrix):\n",
        "    matrix = matrix + torch.eye(matrix.size(0))\n",
        "    row_sum = matrix.sum(1)\n",
        "    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
        "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
        "    normalized_laplacian = (\n",
        "        matrix.matmul(d_mat_inv_sqrt).transpose(0, 1).matmul(d_mat_inv_sqrt)\n",
        "    )\n",
        "    return normalized_laplacian"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TGCN model"
      ],
      "metadata": {
        "id": "QFNkEsKohV95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QLMNipJGGVm"
      },
      "outputs": [],
      "source": [
        "class TGCNConv(nn.Module):\n",
        "    def __init__(self, adj, gru_num: int, out_dim: int, bias: float = 0.0):\n",
        "        super(TGCNConv, self).__init__()\n",
        "        self._gru_num = gru_num\n",
        "        self._out_dim = out_dim\n",
        "        self._bias_init_value = bias\n",
        "        self.register_buffer(\n",
        "            \"laplacian\", calculate_laplacian_with_self_loop(torch.FloatTensor(adj))\n",
        "        )\n",
        "        self.weights = nn.Parameter(\n",
        "            torch.FloatTensor(self._gru_num + 1, self._out_dim)\n",
        "        )\n",
        "        self.biases = nn.Parameter(torch.FloatTensor(self._out_dim))\n",
        "        self.param_reset()\n",
        "\n",
        "    def param_reset(self):\n",
        "        nn.init.xavier_uniform_(self.weights)\n",
        "        nn.init.constant_(self.biases, self._bias_init_value)\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        batch_size = inputs.shape[0]\n",
        "        num_nodes = inputs.shape[1]\n",
        "        # Reshape inputs to size [batch size, number nodes, 1]\n",
        "        inputs = inputs.reshape((batch_size, num_nodes, 1))\n",
        "        # Reshape hidden layer size to [batch_size, num_nodes, gru_num]\n",
        "        hidden = hidden.reshape(\n",
        "            (batch_size, num_nodes, self._gru_num)\n",
        "        )\n",
        "        # As seen in the equations in the paper, we require [x, h] which is concatinate of inputs x and hidden layers h\n",
        "        concat = torch.cat((inputs, hidden), dim=2)\n",
        "        concat = concat.transpose(0, 1).transpose(1, 2)\n",
        "        concat = concat.reshape(\n",
        "            (num_nodes, (self._gru_num + 1) * batch_size)\n",
        "        )\n",
        "        # Concat adjacency matrix A with [x, h]\n",
        "        concat_a = self.laplacian @ concat\n",
        "        concat_a = concat_a.reshape(\n",
        "            (num_nodes, self._gru_num + 1, batch_size)\n",
        "        )\n",
        "        concat_a = concat_a.transpose(0, 2).transpose(1, 2)\n",
        "        concat_a = concat_a.reshape(\n",
        "            (batch_size * num_nodes, self._gru_num + 1)\n",
        "        )\n",
        "        # Multiply A[x, h] with weights W and add bias\n",
        "        out = concat_a @ self.weights + self.biases\n",
        "        out = out.reshape((batch_size, num_nodes, self._out_dim))\n",
        "        out = out.reshape((batch_size, num_nodes * self._out_dim))\n",
        "        return out\n",
        "\n",
        "class TGCNCell(nn.Module):\n",
        "    def __init__(self, adj, input_dim: int, hidden_dim: int):\n",
        "        super(TGCNCell, self).__init__()\n",
        "        self._input_dim = input_dim\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.FloatTensor(adj))\n",
        "        self.graph_conv1 = TGCNConv(\n",
        "            self.adj, self._hidden_dim, self._hidden_dim * 2, bias=1.0\n",
        "        )\n",
        "        self.graph_conv2 = TGCNConv(\n",
        "            self.adj, self._hidden_dim, self._hidden_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, hidden):\n",
        "        concat = torch.sigmoid(self.graph_conv1(inputs, hidden))\n",
        "        # Calculate u_t = sigma(W_u[f(A, X_t), h_{t-1}] +b_u)\n",
        "        # Calculate r_t = sigma(W_r[f(A, X_t), h_{t-1}] +b_r)\n",
        "\n",
        "        r_t, u_t = torch.chunk(concat, chunks=2, dim=1)\n",
        "        # Calculate c_t = tanh(W_u[f(A, X_t), (r_t * h_{t-1})] +b_c)\n",
        "\n",
        "        c_t = torch.tanh(self.graph_conv2(inputs, r_t*hidden))\n",
        "        # Calculate h_t = u_t * h_{t-1} + (1-u_t)*c_t\n",
        "\n",
        "        h_t = u_t * hidden + (1.0 - u_t) * c_t\n",
        "        return h_t, h_t\n",
        "\n",
        "class TGCN(nn.Module):\n",
        "    def __init__(self, adj, hidden_dim: int, **kwargs):\n",
        "        super(TGCN, self).__init__()\n",
        "        # Combine TGCN convolution and cell together to form T-GCN\n",
        "        self._input_dim = adj.shape[0]\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.FloatTensor(adj))\n",
        "        self.tgcn_cell = TGCNCell(self.adj, self._input_dim, self._hidden_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, seq_len, num_nodes = inputs.shape\n",
        "        assert self._input_dim == num_nodes\n",
        "        hidden = torch.zeros(batch_size, num_nodes * self._hidden_dim).type_as(\n",
        "            inputs\n",
        "        )\n",
        "        output = None\n",
        "        for i in range(seq_len):\n",
        "            output, hidden = self.tgcn_cell(inputs[:, i, :], hidden)\n",
        "            output = output.reshape((batch_size, num_nodes, self._hidden_dim))\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "NGZh_VCgE5Uj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize T-GCN model"
      ],
      "metadata": {
        "id": "wXavdaLnJn3t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRZb8i2laiPq"
      },
      "outputs": [],
      "source": [
        "tgcn = TGCN(adj=adj, hidden_dim=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup Adam optimizer and loss criterion"
      ],
      "metadata": {
        "id": "O9rXmvG_Jpg7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F34MR-rBin_l",
        "outputId": "d0150ba5-3cb6-4e26-9b8e-4e237323558a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TGCN(\n",
            "  (tgcn_cell): TGCNCell(\n",
            "    (graph_conv1): TGCNGraphConvolution()\n",
            "    (graph_conv2): TGCNGraphConvolution()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(tgcn.parameters(), lr=1e-2, weight_decay=1.5e-3)\n",
        "criterion = torch.nn.MSELoss()\n",
        "print(tgcn)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss function and metric calculations"
      ],
      "metadata": {
        "id": "nSHodwKq1eoy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XvxoTk6NDt5"
      },
      "outputs": [],
      "source": [
        "def mse_with_regularizer_loss_func(inputs, targets, model, lamda=1.5e-3):\n",
        "    reg_loss = 0.0\n",
        "    for param in model.parameters():\n",
        "        reg_loss += torch.sum(param ** 2) / 2\n",
        "    reg_loss = lamda * reg_loss\n",
        "    mse_loss = torch.sum((inputs - targets) ** 2) / 2\n",
        "    return mse_loss + reg_loss\n",
        "\n",
        "def mse_loss(pred, y):\n",
        "    return torch.sum((pred - y) ** 2) / len(pred)\n",
        "\n",
        "def mae_loss(pred, y):\n",
        "    return torch.sum(torch.abs(pred - y)) / len(pred)\n",
        "\n",
        "def accuracy_func(pred, y):\n",
        "    return 1 - torch.linalg.norm(y - pred, \"fro\") / torch.linalg.norm(y, \"fro\")\n",
        "\n",
        "\n",
        "def r2_func(pred, y):\n",
        "    return 1 - torch.sum((y - pred) ** 2) / torch.sum((y - torch.mean(pred)) ** 2)\n",
        "\n",
        "\n",
        "def explained_variance_func(pred, y):\n",
        "    return 1 - torch.var(y - pred) / torch.var(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "SnBm_eS1Jt7e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtZiAR_QkB6-",
        "outputId": "f9e80558-4162-4216-dc04-6d42c261d203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,     1] loss: 311789.219\n",
            "[1,     6] loss: 104141.984\n",
            "[1,    11] loss: 23942.793\n",
            "[1,    16] loss: 12053.276\n",
            "[1,    21] loss: 21569.061\n",
            "[EPOCH 1] average loss: 14039.232\n",
            "[2,     1] loss: 25633.531\n",
            "[2,     6] loss: 24866.072\n",
            "[2,    11] loss: 21344.463\n",
            "[2,    16] loss: 12125.839\n",
            "[2,    21] loss: 21337.639\n",
            "[EPOCH 2] average loss: 13500.263\n",
            "[3,     1] loss: 24929.043\n",
            "[3,     6] loss: 23725.199\n",
            "[3,    11] loss: 20774.510\n",
            "[3,    16] loss: 11693.146\n",
            "[3,    21] loss: 20261.879\n",
            "[EPOCH 3] average loss: 12783.668\n",
            "[4,     1] loss: 23424.777\n",
            "[4,     6] loss: 22508.438\n",
            "[4,    11] loss: 19830.211\n",
            "[4,    16] loss: 11428.730\n",
            "[4,    21] loss: 19391.941\n",
            "[EPOCH 4] average loss: 12256.950\n",
            "[5,     1] loss: 22518.689\n",
            "[5,     6] loss: 21313.461\n",
            "[5,    11] loss: 19520.996\n",
            "[5,    16] loss: 11212.618\n",
            "[5,    21] loss: 19462.783\n",
            "[EPOCH 5] average loss: 12325.096\n",
            "[6,     1] loss: 22350.164\n",
            "[6,     6] loss: 21188.531\n",
            "[6,    11] loss: 19177.211\n",
            "[6,    16] loss: 11171.752\n",
            "[6,    21] loss: 19341.641\n",
            "[EPOCH 6] average loss: 12239.289\n",
            "[7,     1] loss: 22364.064\n",
            "[7,     6] loss: 21028.238\n",
            "[7,    11] loss: 18655.023\n",
            "[7,    16] loss: 11155.479\n",
            "[7,    21] loss: 18732.334\n",
            "[EPOCH 7] average loss: 11947.286\n",
            "[8,     1] loss: 21876.807\n",
            "[8,     6] loss: 20192.217\n",
            "[8,    11] loss: 18365.256\n",
            "[8,    16] loss: 10933.654\n",
            "[8,    21] loss: 18334.133\n",
            "[EPOCH 8] average loss: 11703.860\n",
            "[9,     1] loss: 21495.309\n",
            "[9,     6] loss: 19864.379\n",
            "[9,    11] loss: 18377.633\n",
            "[9,    16] loss: 10907.523\n",
            "[9,    21] loss: 18541.939\n",
            "[EPOCH 9] average loss: 11860.454\n",
            "[10,     1] loss: 21788.887\n",
            "[10,     6] loss: 19879.480\n",
            "[10,    11] loss: 18188.508\n",
            "[10,    16] loss: 11056.104\n",
            "[10,    21] loss: 18388.408\n",
            "[EPOCH 10] average loss: 11794.949\n",
            "[11,     1] loss: 21788.314\n",
            "[11,     6] loss: 19941.660\n",
            "[11,    11] loss: 18331.477\n",
            "[11,    16] loss: 11004.238\n",
            "[11,    21] loss: 18373.383\n",
            "[EPOCH 11] average loss: 11823.679\n",
            "[12,     1] loss: 21693.705\n",
            "[12,     6] loss: 19590.340\n",
            "[12,    11] loss: 17914.469\n",
            "[12,    16] loss: 10986.351\n",
            "[12,    21] loss: 17896.791\n",
            "[EPOCH 12] average loss: 11610.654\n",
            "[13,     1] loss: 21281.561\n",
            "[13,     6] loss: 19131.885\n",
            "[13,    11] loss: 17775.102\n",
            "[13,    16] loss: 10912.235\n",
            "[13,    21] loss: 17861.941\n",
            "[EPOCH 13] average loss: 11626.296\n",
            "[14,     1] loss: 21104.957\n",
            "[14,     6] loss: 18890.457\n",
            "[14,    11] loss: 17776.852\n",
            "[14,    16] loss: 10780.804\n",
            "[14,    21] loss: 17793.820\n",
            "[EPOCH 14] average loss: 11658.411\n",
            "[15,     1] loss: 21272.771\n",
            "[15,     6] loss: 18838.354\n",
            "[15,    11] loss: 17488.092\n",
            "[15,    16] loss: 10911.761\n",
            "[15,    21] loss: 17662.682\n",
            "[EPOCH 15] average loss: 11567.502\n",
            "[16,     1] loss: 21191.209\n",
            "[16,     6] loss: 18625.340\n",
            "[16,    11] loss: 17457.113\n",
            "[16,    16] loss: 10808.245\n",
            "[16,    21] loss: 17750.697\n",
            "[EPOCH 16] average loss: 11648.603\n",
            "[17,     1] loss: 21498.613\n",
            "[17,     6] loss: 18692.801\n",
            "[17,    11] loss: 17651.709\n",
            "[17,    16] loss: 10759.567\n",
            "[17,    21] loss: 17302.932\n",
            "[EPOCH 17] average loss: 11405.309\n",
            "[18,     1] loss: 20977.283\n",
            "[18,     6] loss: 18246.629\n",
            "[18,    11] loss: 17332.039\n",
            "[18,    16] loss: 10792.209\n",
            "[18,    21] loss: 17299.174\n",
            "[EPOCH 18] average loss: 11388.496\n",
            "[19,     1] loss: 20966.293\n",
            "[19,     6] loss: 18405.928\n",
            "[19,    11] loss: 17564.627\n",
            "[19,    16] loss: 10608.471\n",
            "[19,    21] loss: 17437.418\n",
            "[EPOCH 19] average loss: 11513.886\n",
            "[20,     1] loss: 21172.873\n",
            "[20,     6] loss: 18377.840\n",
            "[20,    11] loss: 17186.244\n",
            "[20,    16] loss: 10636.283\n",
            "[20,    21] loss: 17241.727\n",
            "[EPOCH 20] average loss: 11399.531\n",
            "[21,     1] loss: 21158.721\n",
            "[21,     6] loss: 18170.617\n",
            "[21,    11] loss: 17315.010\n",
            "[21,    16] loss: 10505.218\n",
            "[21,    21] loss: 17596.072\n",
            "[EPOCH 21] average loss: 11598.353\n",
            "[22,     1] loss: 21529.008\n",
            "[22,     6] loss: 18365.305\n",
            "[22,    11] loss: 17172.609\n",
            "[22,    16] loss: 10430.436\n",
            "[22,    21] loss: 17180.158\n",
            "[EPOCH 22] average loss: 11244.935\n",
            "[23,     1] loss: 20997.045\n",
            "[23,     6] loss: 18118.660\n",
            "[23,    11] loss: 17019.818\n",
            "[23,    16] loss: 10418.136\n",
            "[23,    21] loss: 16844.008\n",
            "[EPOCH 23] average loss: 10985.909\n",
            "[24,     1] loss: 20367.221\n",
            "[24,     6] loss: 22784.359\n",
            "[24,    11] loss: 30221.111\n",
            "[24,    16] loss: 20842.273\n",
            "[24,    21] loss: 24518.275\n",
            "[EPOCH 24] average loss: 15059.923\n",
            "[25,     1] loss: 26939.939\n",
            "[25,     6] loss: 26423.359\n",
            "[25,    11] loss: 21265.291\n",
            "[25,    16] loss: 12292.923\n",
            "[25,    21] loss: 20850.238\n",
            "[EPOCH 25] average loss: 13660.879\n",
            "[26,     1] loss: 24860.145\n",
            "[26,     6] loss: 24389.111\n",
            "[26,    11] loss: 20346.014\n",
            "[26,    16] loss: 12110.207\n",
            "[26,    21] loss: 19628.465\n",
            "[EPOCH 26] average loss: 13082.718\n",
            "[27,     1] loss: 23957.648\n",
            "[27,     6] loss: 23550.682\n",
            "[27,    11] loss: 19927.393\n",
            "[27,    16] loss: 12095.493\n",
            "[27,    21] loss: 19200.027\n",
            "[EPOCH 27] average loss: 12982.066\n",
            "[28,     1] loss: 23898.885\n",
            "[28,     6] loss: 23327.771\n",
            "[28,    11] loss: 19659.131\n",
            "[28,    16] loss: 12230.818\n",
            "[28,    21] loss: 18567.660\n",
            "[EPOCH 28] average loss: 12670.675\n",
            "[29,     1] loss: 23407.822\n",
            "[29,     6] loss: 22708.916\n",
            "[29,    11] loss: 19571.314\n",
            "[29,    16] loss: 12003.160\n",
            "[29,    21] loss: 18444.410\n",
            "[EPOCH 29] average loss: 12582.544\n",
            "[30,     1] loss: 23191.846\n",
            "[30,     6] loss: 22344.383\n",
            "[30,    11] loss: 19082.264\n",
            "[30,    16] loss: 11826.520\n",
            "[30,    21] loss: 17728.141\n",
            "[EPOCH 30] average loss: 12204.516\n",
            "[31,     1] loss: 22523.059\n",
            "[31,     6] loss: 21764.578\n",
            "[31,    11] loss: 18580.848\n",
            "[31,    16] loss: 11560.442\n",
            "[31,    21] loss: 17058.764\n",
            "[EPOCH 31] average loss: 11790.521\n",
            "[32,     1] loss: 21644.924\n",
            "[32,     6] loss: 20893.197\n",
            "[32,    11] loss: 18036.170\n",
            "[32,    16] loss: 11496.467\n",
            "[32,    21] loss: 16238.271\n",
            "[EPOCH 32] average loss: 11289.051\n",
            "[33,     1] loss: 20788.469\n",
            "[33,     6] loss: 20151.301\n",
            "[33,    11] loss: 17848.570\n",
            "[33,    16] loss: 11188.620\n",
            "[33,    21] loss: 16386.779\n",
            "[EPOCH 33] average loss: 11195.251\n",
            "[34,     1] loss: 20744.361\n",
            "[34,     6] loss: 19647.637\n",
            "[34,    11] loss: 17829.174\n",
            "[34,    16] loss: 11130.423\n",
            "[34,    21] loss: 15982.268\n",
            "[EPOCH 34] average loss: 11100.440\n",
            "[35,     1] loss: 20854.918\n",
            "[35,     6] loss: 19474.586\n",
            "[35,    11] loss: 17514.543\n",
            "[35,    16] loss: 11243.901\n",
            "[35,    21] loss: 15756.484\n",
            "[EPOCH 35] average loss: 11076.548\n",
            "[36,     1] loss: 20767.648\n",
            "[36,     6] loss: 19291.611\n",
            "[36,    11] loss: 17201.980\n",
            "[36,    16] loss: 11217.819\n",
            "[36,    21] loss: 15219.642\n",
            "[EPOCH 36] average loss: 10714.576\n",
            "[37,     1] loss: 20359.047\n",
            "[37,     6] loss: 19118.744\n",
            "[37,    11] loss: 16952.982\n",
            "[37,    16] loss: 11069.252\n",
            "[37,    21] loss: 14784.080\n",
            "[EPOCH 37] average loss: 10420.449\n",
            "[38,     1] loss: 19618.266\n",
            "[38,     6] loss: 18484.189\n",
            "[38,    11] loss: 16847.986\n",
            "[38,    16] loss: 11057.933\n",
            "[38,    21] loss: 14951.387\n",
            "[EPOCH 38] average loss: 10560.215\n",
            "[39,     1] loss: 19975.928\n",
            "[39,     6] loss: 18743.490\n",
            "[39,    11] loss: 16660.863\n",
            "[39,    16] loss: 11034.587\n",
            "[39,    21] loss: 14567.462\n",
            "[EPOCH 39] average loss: 10281.199\n",
            "[40,     1] loss: 19577.023\n",
            "[40,     6] loss: 18356.732\n",
            "[40,    11] loss: 16659.047\n",
            "[40,    16] loss: 10954.274\n",
            "[40,    21] loss: 14595.660\n",
            "[EPOCH 40] average loss: 10298.111\n",
            "[41,     1] loss: 19615.051\n",
            "[41,     6] loss: 18402.762\n",
            "[41,    11] loss: 16608.742\n",
            "[41,    16] loss: 10929.882\n",
            "[41,    21] loss: 14509.525\n",
            "[EPOCH 41] average loss: 10217.562\n",
            "[42,     1] loss: 19515.818\n",
            "[42,     6] loss: 18262.686\n",
            "[42,    11] loss: 16584.521\n",
            "[42,    16] loss: 10901.336\n",
            "[42,    21] loss: 14521.477\n",
            "[EPOCH 42] average loss: 10221.181\n",
            "[43,     1] loss: 19531.633\n",
            "[43,     6] loss: 18271.016\n",
            "[43,    11] loss: 16524.961\n",
            "[43,    16] loss: 10891.618\n",
            "[43,    21] loss: 14461.747\n",
            "[EPOCH 43] average loss: 10163.727\n",
            "[44,     1] loss: 19482.693\n",
            "[44,     6] loss: 18200.158\n",
            "[44,    11] loss: 16496.748\n",
            "[44,    16] loss: 10856.656\n",
            "[44,    21] loss: 14457.946\n",
            "[EPOCH 44] average loss: 10138.208\n",
            "[45,     1] loss: 19464.064\n",
            "[45,     6] loss: 18137.508\n",
            "[45,    11] loss: 16464.449\n",
            "[45,    16] loss: 10811.416\n",
            "[45,    21] loss: 14493.249\n",
            "[EPOCH 45] average loss: 10160.160\n",
            "[46,     1] loss: 19608.363\n",
            "[46,     6] loss: 18050.279\n",
            "[46,    11] loss: 16497.770\n",
            "[46,    16] loss: 10763.241\n",
            "[46,    21] loss: 14439.148\n",
            "[EPOCH 46] average loss: 10074.862\n",
            "[47,     1] loss: 19517.684\n",
            "[47,     6] loss: 17846.262\n",
            "[47,    11] loss: 16694.074\n",
            "[47,    16] loss: 10698.590\n",
            "[47,    21] loss: 14644.149\n",
            "[EPOCH 47] average loss: 10288.431\n",
            "[48,     1] loss: 20109.852\n",
            "[48,     6] loss: 18225.994\n",
            "[48,    11] loss: 16677.123\n",
            "[48,    16] loss: 10662.609\n",
            "[48,    21] loss: 14895.381\n",
            "[EPOCH 48] average loss: 10457.739\n",
            "[49,     1] loss: 20274.145\n",
            "[49,     6] loss: 18200.646\n",
            "[49,    11] loss: 16572.234\n",
            "[49,    16] loss: 10614.930\n",
            "[49,    21] loss: 14621.579\n",
            "[EPOCH 49] average loss: 10227.124\n",
            "[50,     1] loss: 19910.254\n",
            "[50,     6] loss: 17937.783\n",
            "[50,    11] loss: 16445.371\n",
            "[50,    16] loss: 10615.727\n",
            "[50,    21] loss: 14608.723\n",
            "[EPOCH 50] average loss: 10289.733\n",
            "[51,     1] loss: 20119.305\n",
            "[51,     6] loss: 18138.709\n",
            "[51,    11] loss: 16344.118\n",
            "[51,    16] loss: 10693.573\n",
            "[51,    21] loss: 14287.090\n",
            "[EPOCH 51] average loss: 10067.545\n",
            "[52,     1] loss: 19743.689\n",
            "[52,     6] loss: 17905.109\n",
            "[52,    11] loss: 16284.062\n",
            "[52,    16] loss: 10569.497\n",
            "[52,    21] loss: 14299.326\n",
            "[EPOCH 52] average loss: 10053.847\n",
            "[53,     1] loss: 19773.760\n",
            "[53,     6] loss: 17852.754\n",
            "[53,    11] loss: 16266.061\n",
            "[53,    16] loss: 10548.242\n",
            "[53,    21] loss: 14266.715\n",
            "[EPOCH 53] average loss: 10040.389\n",
            "[54,     1] loss: 19782.781\n",
            "[54,     6] loss: 17852.863\n",
            "[54,    11] loss: 16224.217\n",
            "[54,    16] loss: 10545.758\n",
            "[54,    21] loss: 14170.979\n",
            "[EPOCH 54] average loss: 9972.325\n",
            "[55,     1] loss: 19709.197\n",
            "[55,     6] loss: 17779.576\n",
            "[55,    11] loss: 16196.085\n",
            "[55,    16] loss: 10490.021\n",
            "[55,    21] loss: 14156.654\n",
            "[EPOCH 55] average loss: 9952.039\n",
            "[56,     1] loss: 19716.406\n",
            "[56,     6] loss: 17758.408\n",
            "[56,    11] loss: 16172.137\n",
            "[56,    16] loss: 10477.625\n",
            "[56,    21] loss: 14094.304\n",
            "[EPOCH 56] average loss: 9906.661\n",
            "[57,     1] loss: 19664.926\n",
            "[57,     6] loss: 17683.092\n",
            "[57,    11] loss: 16145.909\n",
            "[57,    16] loss: 10403.560\n",
            "[57,    21] loss: 14115.041\n",
            "[EPOCH 57] average loss: 9904.899\n",
            "[58,     1] loss: 19711.645\n",
            "[58,     6] loss: 17694.189\n",
            "[58,    11] loss: 16135.578\n",
            "[58,    16] loss: 10421.156\n",
            "[58,    21] loss: 14031.705\n",
            "[EPOCH 58] average loss: 9850.632\n",
            "[59,     1] loss: 19628.643\n",
            "[59,     6] loss: 17602.908\n",
            "[59,    11] loss: 16098.470\n",
            "[59,    16] loss: 10338.879\n",
            "[59,    21] loss: 14062.945\n",
            "[EPOCH 59] average loss: 9855.953\n",
            "[60,     1] loss: 19699.586\n",
            "[60,     6] loss: 17626.574\n",
            "[60,    11] loss: 16086.604\n",
            "[60,    16] loss: 10361.539\n",
            "[60,    21] loss: 13974.615\n",
            "[EPOCH 60] average loss: 9798.426\n",
            "[61,     1] loss: 19605.426\n",
            "[61,     6] loss: 17530.955\n",
            "[61,    11] loss: 16047.552\n",
            "[61,    16] loss: 10287.314\n",
            "[61,    21] loss: 14008.899\n",
            "[EPOCH 61] average loss: 9809.559\n",
            "[62,     1] loss: 19681.750\n",
            "[62,     6] loss: 17557.439\n",
            "[62,    11] loss: 16031.409\n",
            "[62,    16] loss: 10301.902\n",
            "[62,    21] loss: 13921.830\n",
            "[EPOCH 62] average loss: 9748.701\n",
            "[63,     1] loss: 19585.990\n",
            "[63,     6] loss: 17459.406\n",
            "[63,    11] loss: 16001.960\n",
            "[63,    16] loss: 10224.338\n",
            "[63,    21] loss: 13966.039\n",
            "[EPOCH 63] average loss: 9766.932\n",
            "[64,     1] loss: 19667.406\n",
            "[64,     6] loss: 17490.434\n",
            "[64,    11] loss: 15985.124\n",
            "[64,    16] loss: 10244.414\n",
            "[64,    21] loss: 13877.471\n",
            "[EPOCH 64] average loss: 9704.392\n",
            "[65,     1] loss: 19571.547\n",
            "[65,     6] loss: 17394.389\n",
            "[65,    11] loss: 15958.427\n",
            "[65,    16] loss: 10167.960\n",
            "[65,    21] loss: 13919.859\n",
            "[EPOCH 65] average loss: 9722.547\n",
            "[66,     1] loss: 19650.000\n",
            "[66,     6] loss: 17423.283\n",
            "[66,    11] loss: 15938.380\n",
            "[66,    16] loss: 10186.204\n",
            "[66,    21] loss: 13835.390\n",
            "[EPOCH 66] average loss: 9661.934\n",
            "[67,     1] loss: 19559.494\n",
            "[67,     6] loss: 17330.189\n",
            "[67,    11] loss: 15915.347\n",
            "[67,    16] loss: 10110.628\n",
            "[67,    21] loss: 13878.351\n",
            "[EPOCH 67] average loss: 9680.915\n",
            "[68,     1] loss: 19635.029\n",
            "[68,     6] loss: 17358.033\n",
            "[68,    11] loss: 15893.849\n",
            "[68,    16] loss: 10130.160\n",
            "[68,    21] loss: 13795.409\n",
            "[EPOCH 68] average loss: 9621.389\n",
            "[69,     1] loss: 19548.439\n",
            "[69,     6] loss: 17269.484\n",
            "[69,    11] loss: 15872.832\n",
            "[69,    16] loss: 10056.488\n",
            "[69,    21] loss: 13835.693\n",
            "[EPOCH 69] average loss: 9639.035\n",
            "[70,     1] loss: 19618.562\n",
            "[70,     6] loss: 17293.383\n",
            "[70,    11] loss: 15849.508\n",
            "[70,    16] loss: 10074.510\n",
            "[70,    21] loss: 13756.791\n",
            "[EPOCH 70] average loss: 9581.995\n",
            "[71,     1] loss: 19538.074\n",
            "[71,     6] loss: 17208.982\n",
            "[71,    11] loss: 15831.527\n",
            "[71,    16] loss: 10001.084\n",
            "[71,    21] loss: 13796.624\n",
            "[EPOCH 71] average loss: 9599.559\n",
            "[72,     1] loss: 19604.041\n",
            "[72,     6] loss: 17231.070\n",
            "[72,    11] loss: 15807.079\n",
            "[72,    16] loss: 10020.772\n",
            "[72,    21] loss: 13719.537\n",
            "[EPOCH 72] average loss: 9543.801\n",
            "[73,     1] loss: 19527.914\n",
            "[73,     6] loss: 17150.645\n",
            "[73,    11] loss: 15791.381\n",
            "[73,    16] loss: 9948.199\n",
            "[73,    21] loss: 13756.844\n",
            "[EPOCH 73] average loss: 9560.366\n",
            "[74,     1] loss: 19588.307\n",
            "[74,     6] loss: 17169.814\n",
            "[74,    11] loss: 15764.415\n",
            "[74,    16] loss: 9968.145\n",
            "[74,    21] loss: 13682.443\n",
            "[EPOCH 74] average loss: 9506.018\n",
            "[75,     1] loss: 19517.873\n",
            "[75,     6] loss: 17093.547\n",
            "[75,    11] loss: 15752.073\n",
            "[75,    16] loss: 9896.330\n",
            "[75,    21] loss: 13717.272\n",
            "[EPOCH 75] average loss: 9521.786\n",
            "[76,     1] loss: 19570.684\n",
            "[76,     6] loss: 17109.600\n",
            "[76,    11] loss: 15722.304\n",
            "[76,    16] loss: 9916.830\n",
            "[76,    21] loss: 13645.292\n",
            "[EPOCH 76] average loss: 9468.388\n",
            "[77,     1] loss: 19507.229\n",
            "[77,     6] loss: 17039.318\n",
            "[77,    11] loss: 15714.162\n",
            "[77,    16] loss: 9846.376\n",
            "[77,    21] loss: 13675.980\n",
            "[EPOCH 77] average loss: 9482.525\n",
            "[78,     1] loss: 19547.570\n",
            "[78,     6] loss: 17049.326\n",
            "[78,    11] loss: 15680.835\n",
            "[78,    16] loss: 9864.234\n",
            "[78,    21] loss: 13610.063\n",
            "[EPOCH 78] average loss: 9431.785\n",
            "[79,     1] loss: 19497.598\n",
            "[79,     6] loss: 16987.900\n",
            "[79,    11] loss: 15679.274\n",
            "[79,    16] loss: 9796.093\n",
            "[79,    21] loss: 13636.266\n",
            "[EPOCH 79] average loss: 9444.453\n",
            "[80,     1] loss: 19520.396\n",
            "[80,     6] loss: 16986.400\n",
            "[80,    11] loss: 15641.779\n",
            "[80,    16] loss: 9805.051\n",
            "[80,    21] loss: 13583.520\n",
            "[EPOCH 80] average loss: 9399.680\n",
            "[81,     1] loss: 19495.863\n",
            "[81,     6] loss: 16940.398\n",
            "[81,    11] loss: 15648.844\n",
            "[81,    16] loss: 9747.311\n",
            "[81,    21] loss: 13600.616\n",
            "[EPOCH 81] average loss: 9409.504\n",
            "[82,     1] loss: 19495.607\n",
            "[82,     6] loss: 16925.848\n",
            "[82,    11] loss: 15606.445\n",
            "[82,    16] loss: 9749.344\n",
            "[82,    21] loss: 13556.427\n",
            "[EPOCH 82] average loss: 9368.938\n",
            "[83,     1] loss: 19488.643\n",
            "[83,     6] loss: 16888.266\n",
            "[83,    11] loss: 15616.719\n",
            "[83,    16] loss: 9701.497\n",
            "[83,    21] loss: 13570.300\n",
            "[EPOCH 83] average loss: 9379.608\n",
            "[84,     1] loss: 19488.824\n",
            "[84,     6] loss: 16885.059\n",
            "[84,    11] loss: 15572.899\n",
            "[84,    16] loss: 9726.098\n",
            "[84,    21] loss: 13504.114\n",
            "[EPOCH 84] average loss: 9330.820\n",
            "[85,     1] loss: 19448.367\n",
            "[85,     6] loss: 16825.348\n",
            "[85,    11] loss: 15570.865\n",
            "[85,    16] loss: 9652.008\n",
            "[85,    21] loss: 13540.738\n",
            "[EPOCH 85] average loss: 9349.492\n",
            "[86,     1] loss: 19481.289\n",
            "[86,     6] loss: 16835.014\n",
            "[86,    11] loss: 15534.039\n",
            "[86,    16] loss: 9679.791\n",
            "[86,    21] loss: 13473.670\n",
            "[EPOCH 86] average loss: 9300.102\n",
            "[87,     1] loss: 19437.787\n",
            "[87,     6] loss: 16774.627\n",
            "[87,    11] loss: 15536.245\n",
            "[87,    16] loss: 9608.742\n",
            "[87,    21] loss: 13504.818\n",
            "[EPOCH 87] average loss: 9317.543\n",
            "[88,     1] loss: 19472.504\n",
            "[88,     6] loss: 16787.838\n",
            "[88,    11] loss: 15506.087\n",
            "[88,    16] loss: 9636.770\n",
            "[88,    21] loss: 13437.599\n",
            "[EPOCH 88] average loss: 9268.458\n",
            "[89,     1] loss: 19410.900\n",
            "[89,     6] loss: 16718.277\n",
            "[89,    11] loss: 15498.403\n",
            "[89,    16] loss: 9564.898\n",
            "[89,    21] loss: 13475.168\n",
            "[EPOCH 89] average loss: 9286.853\n",
            "[90,     1] loss: 19455.936\n",
            "[90,     6] loss: 16736.070\n",
            "[90,    11] loss: 15467.578\n",
            "[90,    16] loss: 9597.209\n",
            "[90,    21] loss: 13403.597\n",
            "[EPOCH 90] average loss: 9237.603\n",
            "[91,     1] loss: 19399.762\n",
            "[91,     6] loss: 16681.438\n",
            "[91,    11] loss: 15467.949\n",
            "[91,    16] loss: 9534.200\n",
            "[91,    21] loss: 13423.340\n",
            "[EPOCH 91] average loss: 9246.513\n",
            "[92,     1] loss: 19417.961\n",
            "[92,     6] loss: 16680.363\n",
            "[92,    11] loss: 15430.563\n",
            "[92,    16] loss: 9553.018\n",
            "[92,    21] loss: 13363.197\n",
            "[EPOCH 92] average loss: 9201.767\n",
            "[93,     1] loss: 19377.301\n",
            "[93,     6] loss: 16633.984\n",
            "[93,    11] loss: 15430.263\n",
            "[93,    16] loss: 9491.616\n",
            "[93,    21] loss: 13384.406\n",
            "[EPOCH 93] average loss: 9209.795\n",
            "[94,     1] loss: 19370.740\n",
            "[94,     6] loss: 16626.836\n",
            "[94,    11] loss: 15392.196\n",
            "[94,    16] loss: 9510.029\n",
            "[94,    21] loss: 13329.677\n",
            "[EPOCH 94] average loss: 9168.003\n",
            "[95,     1] loss: 19367.549\n",
            "[95,     6] loss: 16600.625\n",
            "[95,    11] loss: 15387.655\n",
            "[95,    16] loss: 9463.365\n",
            "[95,    21] loss: 13334.887\n",
            "[EPOCH 95] average loss: 9169.105\n",
            "[96,     1] loss: 19358.801\n",
            "[96,     6] loss: 16589.049\n",
            "[96,    11] loss: 15351.036\n",
            "[96,    16] loss: 9472.274\n",
            "[96,    21] loss: 13287.825\n",
            "[EPOCH 96] average loss: 9132.476\n",
            "[97,     1] loss: 19338.500\n",
            "[97,     6] loss: 16545.920\n",
            "[97,    11] loss: 15346.688\n",
            "[97,    16] loss: 9415.673\n",
            "[97,    21] loss: 13320.425\n",
            "[EPOCH 97] average loss: 9147.188\n",
            "[98,     1] loss: 19353.545\n",
            "[98,     6] loss: 16544.736\n",
            "[98,    11] loss: 15313.552\n",
            "[98,    16] loss: 9436.354\n",
            "[98,    21] loss: 13266.392\n",
            "[EPOCH 98] average loss: 9110.129\n",
            "[99,     1] loss: 19345.611\n",
            "[99,     6] loss: 16515.143\n",
            "[99,    11] loss: 15311.667\n",
            "[99,    16] loss: 9390.313\n",
            "[99,    21] loss: 13285.744\n",
            "[EPOCH 99] average loss: 9120.860\n",
            "[100,     1] loss: 19323.922\n",
            "[100,     6] loss: 16503.645\n",
            "[100,    11] loss: 15280.349\n",
            "[100,    16] loss: 9400.799\n",
            "[100,    21] loss: 13237.190\n",
            "[EPOCH 100] average loss: 9084.058\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "tgcn.train()\n",
        "torch.set_grad_enabled(True)\n",
        "for epoch in range(100):  # loop over the dataset multiple times\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "    for i, batch in enumerate(train_data):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = batch\n",
        "        # zero the parameter gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        pred = tgcn(inputs)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred = pred.transpose(1, 2).reshape((-1, inputs.size(2)))\n",
        "        labels = labels.reshape((-1, labels.size(2)))\n",
        "\n",
        "        # loss = criterion(pred, labels)\n",
        "        loss = mse_with_regularizer_loss_func(pred, labels, tgcn)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        epoch_loss = running_loss\n",
        "        if i % 5 == 0: #print every 5 iters\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "    epoch_loss = epoch_loss/len(batch)\n",
        "    print(f'[EPOCH {epoch + 1}] average loss: {epoch_loss:.3f}')\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "84N7T_25E_pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing cell"
      ],
      "metadata": {
        "id": "V-XPDAjFJwo6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER_eTBqZ0TeL",
        "outputId": "a96c97b9-33d2-44bc-fd99-438ddb1f8d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.1228640079498291\n",
            "R2: 0.12633401155471802\n",
            "MSE: 0.03671204671263695\n",
            "RMSE: 0.191603884100914\n",
            "Acc: 0.7706314325332642\n",
            "Var: 0.13082873821258545\n",
            "val_loss: 79763.8515625\n",
            "my_RMSE: 2.756699800491333\n",
            "my_MAE: 25.432849884033203\n"
          ]
        }
      ],
      "source": [
        "mae = 0\n",
        "tgcn.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_data:\n",
        "        input, labels = data\n",
        "        pred = tgcn(input)\n",
        "        # reshape\n",
        "        pred = pred.transpose(1, 2).reshape((-1, input.size(2)))\n",
        "        labels = labels.reshape((-1, labels.size(2)))\n",
        "\n",
        "\n",
        "        val_loss = mse_with_regularizer_loss_func(pred, labels, tgcn)\n",
        "        mae = torchmetrics.functional.mean_absolute_error(pred, labels)\n",
        "        acc = accuracy_func(pred, labels)\n",
        "        r2 = r2_func(pred, labels)\n",
        "        explained_var = explained_variance_func(pred, labels)\n",
        "        mse = torchmetrics.functional.mean_squared_error(pred, labels)\n",
        "        rmse = torch.sqrt(mse)\n",
        "        my_rmse = torch.sqrt(mse_loss(pred, labels))\n",
        "        my_mae = (mae_loss(pred, labels))\n",
        "        \n",
        "print('MAE: ' + str(mae.item()))\n",
        "print('R2: ' + str(r2.item()))\n",
        "print('MSE: ' + str(mse.item()))\n",
        "print('RMSE: ' + str(rmse.item()))\n",
        "print('Acc: ' + str(acc.item()))\n",
        "print('Var: ' + str(explained_var.item()))\n",
        "print('val_loss: ' + str(val_loss.item()))\n",
        "print('my_RMSE: ' + str(my_rmse.item()))\n",
        "print('my_MAE: ' + str(my_mae.item()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "6322 Project.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}