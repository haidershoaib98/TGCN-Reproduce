{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6322 Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haidershoaib98/TGCN-Reproduce/blob/main/6322_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs"
      ],
      "metadata": {
        "id": "K8gwrZ2vpanj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjYpXF9egsEy",
        "outputId": "88fdc85a-1f4f-4b07-875f-6af6f5607609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libspatialindex-dev is already the newest version (1.8.5-5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: osmnx in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: geopandas>=0.10 in /usr/local/lib/python3.7/dist-packages (from osmnx) (0.10.2)\n",
            "Requirement already satisfied: Shapely<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.8.1.post1)\n",
            "Requirement already satisfied: requests>=2.26 in /usr/local/lib/python3.7/dist-packages (from osmnx) (2.27.1)\n",
            "Requirement already satisfied: Rtree>=0.9 in /usr/local/lib/python3.7/dist-packages (from osmnx) (0.9.7)\n",
            "Requirement already satisfied: matplotlib>=3.4 in /usr/local/lib/python3.7/dist-packages (from osmnx) (3.5.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.21.5)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.7/dist-packages (from osmnx) (1.3.5)\n",
            "Requirement already satisfied: pyproj>=3.2 in /usr/local/lib/python3.7/dist-packages (from osmnx) (3.2.1)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.7/dist-packages (from osmnx) (2.6.3)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas>=0.10->osmnx) (1.8.21)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (7.1.2)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (21.4.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (1.1.1)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (57.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (2021.10.8)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (0.7.2)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas>=0.10->osmnx) (1.15.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (21.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (4.31.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (3.0.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.4->osmnx) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.4->osmnx) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3->osmnx) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->osmnx) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!apt install libspatialindex-dev\n",
        "!pip install osmnx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/PyTorchLightning/pytorch-lightning\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_dMHCzLg3zo",
        "outputId": "aca8eecf-13c0-4980-85ee-1fa5d0485a7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/PyTorchLightning/pytorch-lightning\n",
            "  Cloning https://github.com/PyTorchLightning/pytorch-lightning to /tmp/pip-req-build-sfi041f7\n",
            "  Running command git clone -q https://github.com/PyTorchLightning/pytorch-lightning /tmp/pip-req-build-sfi041f7\n",
            "  Running command git submodule update --init --recursive -q\n",
            "  From https://github.com/PyTorchLightning/lightning-tutorials\n",
            "   * branch            290fb466de1fcc2ac6025f74b56906592911e856 -> FETCH_HEAD\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (1.10.0+cu111)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (6.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (21.3)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (2.8.0)\n",
            "Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (0.7.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (1.21.5)\n",
            "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (0.3.2)\n",
            "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (2022.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (4.1.1)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.7.0.dev0) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch-lightning==1.7.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.44.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.3.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (0.4.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.7.0.dev0) (3.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (4.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (1.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.7.0.dev0) (21.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaxuZeJqFe5j",
        "outputId": "99b15b22-445a-4415-9211-eb1691b70d9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.7/dist-packages (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Python libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as sp\n",
        "import h5py\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.functional import normalize\n",
        "import argparse"
      ],
      "metadata": {
        "id": "PvqKfpaqg-bo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Setup"
      ],
      "metadata": {
        "id": "ajX-IF8tphcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvV3bIfdhXCj",
        "outputId": "e3a0392f-3811-473c-c0fa-7886626df261"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/haidershoaib98/TGCN-Reproduce.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sArL9QxkAHID",
        "outputId": "949f80b1-9d9a-4c2f-e998-4deba37ad1cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'TGCN-Reproduce' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Getting data from github clone"
      ],
      "metadata": {
        "id": "tV12s3u31kT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Los Loop\n",
        "f = pd.read_csv('TGCN-Reproduce/data/los_speed.csv')\n",
        "f_adj = pd.read_csv('TGCN-Reproduce/data/los_adj.csv')"
      ],
      "metadata": {
        "id": "4Khm-XHs1oBv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haider's Drive (we will connect with github later to make this easier)"
      ],
      "metadata": {
        "id": "fFTU9bxnhw49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Los Loop\n",
        "f = pd.read_csv('/content/drive/My Drive/TGCN-Reproduce/data/los_speed.csv')\n",
        "f_adj = pd.read_csv('/content/drive/My Drive/TGCN-Reproduce/data/los_adj.csv')\n"
      ],
      "metadata": {
        "id": "O557j6pnhtJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "los_speed = pd.DataFrame(f)\n",
        "los_speed_array = los_speed.to_numpy()"
      ],
      "metadata": {
        "id": "_I_B9weyiECu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "los_speed_array.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OALoRh14jHYz",
        "outputId": "cce93ad6-89b6-49fc-df02-f70d58788b53"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2016, 207)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T-GCN Model"
      ],
      "metadata": {
        "id": "ikIHjZsBpoqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell creates the dataset into tensors "
      ],
      "metadata": {
        "id": "kOtiZfFouZLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_features(feat_path, dtype=np.float32):\n",
        "    feat_df = pd.read_csv(feat_path)\n",
        "    feat = np.array(feat_df, dtype=dtype)\n",
        "    return feat\n",
        "\n",
        "\n",
        "def load_adjacency_matrix(adj_path, dtype=np.float32):\n",
        "    adj_df = pd.read_csv(adj_path, header=None)\n",
        "    adj = np.array(adj_df, dtype=dtype)\n",
        "    return adj\n",
        "\n",
        "\n",
        "def generate_dataset(\n",
        "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
        "):\n",
        "    \"\"\"\n",
        "    :param data: feature matrix\n",
        "    :param seq_len: length of the train data sequence\n",
        "    :param pre_len: length of the prediction data sequence\n",
        "    :param time_len: length of the time series in total\n",
        "    :param split_ratio: proportion of the training set\n",
        "    :param normalize: scale the data to (0, 1], divide by the maximum value in the data\n",
        "    :return: train set (X, Y) and test set (X, Y)\n",
        "    \"\"\"\n",
        "    if time_len is None:\n",
        "        time_len = data.shape[0]\n",
        "    if normalize:\n",
        "        max_val = np.max(data)\n",
        "        data = data / max_val\n",
        "    train_size = int(time_len * split_ratio)\n",
        "    train_data = data[:train_size]\n",
        "    test_data = data[train_size:time_len]\n",
        "    train_X, train_Y, test_X, test_Y = list(), list(), list(), list()\n",
        "    for i in range(len(train_data) - seq_len - pre_len):\n",
        "        train_X.append(np.array(train_data[i : i + seq_len]))\n",
        "        train_Y.append(np.array(train_data[i + seq_len : i + seq_len + pre_len]))\n",
        "    for i in range(len(test_data) - seq_len - pre_len):\n",
        "        test_X.append(np.array(test_data[i : i + seq_len]))\n",
        "        test_Y.append(np.array(test_data[i + seq_len : i + seq_len + pre_len]))\n",
        "    return np.array(train_X), np.array(train_Y), np.array(test_X), np.array(test_Y)\n",
        "\n",
        "\n",
        "def generate_torch_datasets(\n",
        "    data, seq_len, pre_len, time_len=None, split_ratio=0.8, normalize=True\n",
        "):\n",
        "    train_X, train_Y, test_X, test_Y = generate_dataset(\n",
        "        data,\n",
        "        seq_len,\n",
        "        pre_len,\n",
        "        time_len=time_len,\n",
        "        split_ratio=split_ratio,\n",
        "        normalize=normalize,\n",
        "    )\n",
        "    train_dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(train_X), torch.FloatTensor(train_Y)\n",
        "    )\n",
        "    test_dataset = torch.utils.data.TensorDataset(\n",
        "        torch.FloatTensor(test_X), torch.FloatTensor(test_Y)\n",
        "    )\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "huKQ-A5NRUmU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim\n",
        "import torch.nn.functional as F\n",
        "import torchmetrics\n",
        "from pytorch_lightning.utilities import rank_zero_info\n",
        "import traceback\n",
        "from torch.utils.data.dataloader import DataLoader"
      ],
      "metadata": {
        "id": "0K69wLv2xAWL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data = load_features(\"content/drive/My Drive/TGCN-Reproduce/data/los_speed.csv\")\n",
        "# adj = load_adjacency_matrix(\"/content/drive/My Drive/TGCN-Reproduce/data/los_adj.csv\")\n",
        "data = load_features(\"TGCN-Reproduce/data/los_speed.csv\")\n",
        "adj = load_adjacency_matrix(\"TGCN-Reproduce/data/los_adj.csv\")\n",
        "tr_x, tr_y, tx, ty = generate_dataset(data, 12, 64)\n",
        "train, test = generate_torch_datasets(data, 12, 64)\n",
        "train_data = DataLoader(train, batch_size=64)\n",
        "test_data = DataLoader(test, batch_size=328)"
      ],
      "metadata": {
        "id": "27neyI6OVBam"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B0jyMX6eWxy",
        "outputId": "998ea906-950f-4d80-e76b-7b499d6c6d15"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_laplacian_with_self_loop(matrix):\n",
        "    matrix = matrix + torch.eye(matrix.size(0))\n",
        "    row_sum = matrix.sum(1)\n",
        "    d_inv_sqrt = torch.pow(row_sum, -0.5).flatten()\n",
        "    d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0\n",
        "    d_mat_inv_sqrt = torch.diag(d_inv_sqrt)\n",
        "    normalized_laplacian = (\n",
        "        matrix.matmul(d_mat_inv_sqrt).transpose(0, 1).matmul(d_mat_inv_sqrt)\n",
        "    )\n",
        "    return normalized_laplacian"
      ],
      "metadata": {
        "id": "RQR35-EiF8xg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main TGCN model"
      ],
      "metadata": {
        "id": "6_YNUSSiukR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TGCNGraphConvolution(nn.Module):\n",
        "    def __init__(self, adj, num_gru_units: int, output_dim: int, bias: float = 0.0):\n",
        "        super(TGCNGraphConvolution, self).__init__()\n",
        "        self._num_gru_units = num_gru_units\n",
        "        self._output_dim = output_dim\n",
        "        self._bias_init_value = bias\n",
        "        self.register_buffer(\n",
        "            \"laplacian\", calculate_laplacian_with_self_loop(torch.FloatTensor(adj))\n",
        "        )\n",
        "        self.weights = nn.Parameter(\n",
        "            torch.FloatTensor(self._num_gru_units + 1, self._output_dim)\n",
        "        )\n",
        "        self.biases = nn.Parameter(torch.FloatTensor(self._output_dim))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.weights)\n",
        "        nn.init.constant_(self.biases, self._bias_init_value)\n",
        "\n",
        "    def forward(self, inputs, hidden_state):\n",
        "        batch_size, num_nodes = inputs.shape\n",
        "        # inputs (batch_size, num_nodes) -> (batch_size, num_nodes, 1)\n",
        "        inputs = inputs.reshape((batch_size, num_nodes, 1))\n",
        "        # hidden_state (batch_size, num_nodes, num_gru_units)\n",
        "        hidden_state = hidden_state.reshape(\n",
        "            (batch_size, num_nodes, self._num_gru_units)\n",
        "        )\n",
        "        # [x, h] (batch_size, num_nodes, num_gru_units + 1)\n",
        "        concatenation = torch.cat((inputs, hidden_state), dim=2)\n",
        "        # [x, h] (num_nodes, num_gru_units + 1, batch_size)\n",
        "        concatenation = concatenation.transpose(0, 1).transpose(1, 2)\n",
        "        # [x, h] (num_nodes, (num_gru_units + 1) * batch_size)\n",
        "        concatenation = concatenation.reshape(\n",
        "            (num_nodes, (self._num_gru_units + 1) * batch_size)\n",
        "        )\n",
        "        # A[x, h] (num_nodes, (num_gru_units + 1) * batch_size)\n",
        "        a_times_concat = self.laplacian @ concatenation\n",
        "        # A[x, h] (num_nodes, num_gru_units + 1, batch_size)\n",
        "        a_times_concat = a_times_concat.reshape(\n",
        "            (num_nodes, self._num_gru_units + 1, batch_size)\n",
        "        )\n",
        "        # A[x, h] (batch_size, num_nodes, num_gru_units + 1)\n",
        "        a_times_concat = a_times_concat.transpose(0, 2).transpose(1, 2)\n",
        "        # A[x, h] (batch_size * num_nodes, num_gru_units + 1)\n",
        "        a_times_concat = a_times_concat.reshape(\n",
        "            (batch_size * num_nodes, self._num_gru_units + 1)\n",
        "        )\n",
        "        # A[x, h]W + b (batch_size * num_nodes, output_dim)\n",
        "        outputs = a_times_concat @ self.weights + self.biases\n",
        "        # A[x, h]W + b (batch_size, num_nodes, output_dim)\n",
        "        outputs = outputs.reshape((batch_size, num_nodes, self._output_dim))\n",
        "        # A[x, h]W + b (batch_size, num_nodes * output_dim)\n",
        "        outputs = outputs.reshape((batch_size, num_nodes * self._output_dim))\n",
        "        return outputs\n",
        "\n",
        "    @property\n",
        "    def hyperparameters(self):\n",
        "        return {\n",
        "            \"num_gru_units\": self._num_gru_units,\n",
        "            \"output_dim\": self._output_dim,\n",
        "            \"bias_init_value\": self._bias_init_value,\n",
        "        }\n",
        "\n",
        "\n",
        "class TGCNCell(nn.Module):\n",
        "    def __init__(self, adj, input_dim: int, hidden_dim: int):\n",
        "        super(TGCNCell, self).__init__()\n",
        "        self._input_dim = input_dim\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.FloatTensor(adj))\n",
        "        self.graph_conv1 = TGCNGraphConvolution(\n",
        "            self.adj, self._hidden_dim, self._hidden_dim * 2, bias=1.0\n",
        "        )\n",
        "        self.graph_conv2 = TGCNGraphConvolution(\n",
        "            self.adj, self._hidden_dim, self._hidden_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs, hidden_state):\n",
        "        # [r, u] = sigmoid(A[x, h]W + b)\n",
        "        # [r, u] (batch_size, num_nodes * (2 * num_gru_units))\n",
        "        concatenation = torch.sigmoid(self.graph_conv1(inputs, hidden_state))\n",
        "        # r (batch_size, num_nodes, num_gru_units)\n",
        "        # u (batch_size, num_nodes, num_gru_units)\n",
        "        r, u = torch.chunk(concatenation, chunks=2, dim=1)\n",
        "        # c = tanh(A[x, (r * h)W + b])\n",
        "        # c (batch_size, num_nodes * num_gru_units)\n",
        "        c = torch.tanh(self.graph_conv2(inputs, r * hidden_state))\n",
        "        # h := u * h + (1 - u) * c\n",
        "        # h (batch_size, num_nodes * num_gru_units)\n",
        "        new_hidden_state = u * hidden_state + (1.0 - u) * c\n",
        "        return new_hidden_state, new_hidden_state\n",
        "\n",
        "    @property\n",
        "    def hyperparameters(self):\n",
        "        return {\"input_dim\": self._input_dim, \"hidden_dim\": self._hidden_dim}\n",
        "\n",
        "class TGCN(nn.Module):\n",
        "    def __init__(self, adj, hidden_dim: int, **kwargs):\n",
        "        super(TGCN, self).__init__()\n",
        "        self._input_dim = adj.shape[0]\n",
        "        self._hidden_dim = hidden_dim\n",
        "        self.register_buffer(\"adj\", torch.FloatTensor(adj))\n",
        "        self.tgcn_cell = TGCNCell(self.adj, self._input_dim, self._hidden_dim)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        batch_size, seq_len, num_nodes = inputs.shape\n",
        "        assert self._input_dim == num_nodes\n",
        "        hidden_state = torch.zeros(batch_size, num_nodes * self._hidden_dim).type_as(\n",
        "            inputs\n",
        "        )\n",
        "        output = None\n",
        "        for i in range(seq_len):\n",
        "            output, hidden_state = self.tgcn_cell(inputs[:, i, :], hidden_state)\n",
        "            output = output.reshape((batch_size, num_nodes, self._hidden_dim))\n",
        "        return output\n",
        "    @staticmethod\n",
        "    def add_model_specific_arguments(parent_parser):\n",
        "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument(\"--hidden_dim\", type=int, default=64)\n",
        "        return parser\n",
        "    @property\n",
        "    def hyperparameters(self):\n",
        "        return {\"input_dim\": self._input_dim, \"hidden_dim\": self._hidden_dim}"
      ],
      "metadata": {
        "id": "6QLMNipJGGVm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tgcn = TGCN(adj=adj, hidden_dim=64)"
      ],
      "metadata": {
        "id": "pRZb8i2laiPq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(tgcn.parameters(), lr=1e-3, weight_decay=1.5e-3)\n",
        "criterion = torch.nn.MSELoss()\n",
        "print(tgcn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F34MR-rBin_l",
        "outputId": "594634c3-20d0-4fcd-9a94-489f074a7f2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TGCN(\n",
            "  (tgcn_cell): TGCNCell(\n",
            "    (graph_conv1): TGCNGraphConvolution()\n",
            "    (graph_conv2): TGCNGraphConvolution()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Haider's code for training and testing"
      ],
      "metadata": {
        "id": "LDx2s_Qy2oow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tgcn.train()\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_data, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = tgcn(inputs)\n",
        "\n",
        "        loss = criterion(outputs.transpose(1, 2).reshape((-1, inputs.size(2))), labels.reshape((-1, labels.size(2))))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.3f}')\n",
        "        running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtZiAR_QkB6-",
        "outputId": "b15db2fb-74bd-450d-981b-3a5fe1e210a7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "torch.Size([64, 64, 207])\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "true_val = []\n",
        "p = []\n",
        "mae = 0\n",
        "tgcn.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_data:\n",
        "        j, true_labels = data\n",
        "        pred.append(tgcn(j))\n",
        "        true_val.append(true_labels)\n",
        "        # p.append(mae.item())\n",
        "        \n",
        "\n",
        "mae = torchmetrics.functional.mean_absolute_error(pred, true_val)"
      ],
      "metadata": {
        "id": "N0tMQKnuhg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mae = 0\n",
        "tgcn.eval()\n",
        "with torch.no_grad():\n",
        "    input, labels = next(iter(test_data))\n",
        "    pred = tgcn(input)\n",
        "    mae = torchmetrics.functional.mean_absolute_error(pred.reshape((-1, input.size(2))), labels.reshape((-1, labels.size(2))))\n",
        "print(mae)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ER_eTBqZ0TeL",
        "outputId": "5b726fed-1fa3-4e00-92bd-959b5832eac2"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.1392)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_func(pred, y):\n",
        "    \"\"\"\n",
        "    :param pred: predictions\n",
        "    :param y: ground truth\n",
        "    :return: accuracy, defined as 1 - (norm(y - pred) / norm(y))\n",
        "    \"\"\"\n",
        "    return 1 - torch.linalg.norm(y - pred, \"fro\") / torch.linalg.norm(y, \"fro\")"
      ],
      "metadata": {
        "id": "U0J50EsC3Rb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#metric functions\n",
        "\n",
        "def mse_with_regularizer_loss_func(inputs, targets, model, lamda=1.5e-3):\n",
        "    reg_loss = 0.0\n",
        "    for param in model.parameters():\n",
        "        reg_loss += torch.sum(param ** 2) / 2\n",
        "    reg_loss = lamda * reg_loss\n",
        "    mse_loss = torch.sum((inputs - targets) ** 2) / 2\n",
        "    return mse_loss + reg_loss\n",
        "\n",
        "def accuracy_func(pred, y):\n",
        "    \"\"\"\n",
        "    :param pred: predictions\n",
        "    :param y: ground truth\n",
        "    :return: accuracy, defined as 1 - (norm(y - pred) / norm(y))\n",
        "    \"\"\"\n",
        "    return 1 - torch.linalg.norm(y - pred, \"fro\") / torch.linalg.norm(y, \"fro\")\n",
        "\n",
        "\n",
        "def r2_func(pred, y):\n",
        "    \"\"\"\n",
        "    :param y: ground truth\n",
        "    :param pred: predictions\n",
        "    :return: R square (coefficient of determination)\n",
        "    \"\"\"\n",
        "    return 1 - torch.sum((y - pred) ** 2) / torch.sum((y - torch.mean(pred)) ** 2)\n",
        "\n",
        "\n",
        "def explained_variance_func(pred, y):\n",
        "    return 1 - torch.var(y - pred) / torch.var(y)"
      ],
      "metadata": {
        "id": "7XvxoTk6NDt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOT USING THESE CELLS BELOW. I don't understand how they are training the model"
      ],
      "metadata": {
        "id": "Q54oKitJqdvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metrics"
      ],
      "metadata": {
        "id": "hyBwXD8DVJU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SupervisedForecastTask(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        regressor=\"linear\",\n",
        "        loss=\"mse\",\n",
        "        pre_len: int = 3,\n",
        "        learning_rate: float = 1e-3,\n",
        "        weight_decay: float = 1.5e-3,\n",
        "        feat_max_val: float = 1.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(SupervisedForecastTask, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.model = model\n",
        "        self.regressor = (\n",
        "            nn.Linear(\n",
        "                self.model.hyperparameters.get(\"hidden_dim\")\n",
        "                or self.model.hyperparameters.get(\"output_dim\"),\n",
        "                self.hparams.pre_len,\n",
        "            )\n",
        "            if regressor == \"linear\"\n",
        "            else regressor\n",
        "        )\n",
        "        self._loss = loss\n",
        "        self.feat_max_val = feat_max_val\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, seq_len, num_nodes)\n",
        "        batch_size, _, num_nodes = x.size()\n",
        "        # (batch_size, num_nodes, hidden_dim)\n",
        "        hidden = self.model(x)\n",
        "        # (batch_size * num_nodes, hidden_dim)\n",
        "        hidden = hidden.reshape((-1, hidden.size(2)))\n",
        "        # (batch_size * num_nodes, pre_len)\n",
        "        if self.regressor is not None:\n",
        "            predictions = self.regressor(hidden)\n",
        "        else:\n",
        "            predictions = hidden\n",
        "        predictions = predictions.reshape((batch_size, num_nodes, -1))\n",
        "        return predictions\n",
        "\n",
        "    def shared_step(self, batch, batch_idx):\n",
        "        # (batch_size, seq_len/pre_len, num_nodes)\n",
        "        x, y = batch\n",
        "        num_nodes = x.size(2)\n",
        "        predictions = self(x)\n",
        "        predictions = predictions.transpose(1, 2).reshape((-1, num_nodes))\n",
        "        y = y.reshape((-1, y.size(2)))\n",
        "        return predictions, y\n",
        "\n",
        "    def loss(self, inputs, targets):\n",
        "        if self._loss == \"mse\":\n",
        "            return F.mse_loss(inputs, targets)\n",
        "        if self._loss == \"mse_with_regularizer\":\n",
        "            return mse_with_regularizer_loss_func(inputs, targets, self)\n",
        "        raise NameError(\"Loss not supported:\", self._loss)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        predictions, y = self.shared_step(batch, batch_idx)\n",
        "        loss = self.loss(predictions, y)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        predictions, y = self.shared_step(batch, batch_idx)\n",
        "        predictions = predictions * self.feat_max_val\n",
        "        y = y * self.feat_max_val\n",
        "        loss = self.loss(predictions, y)\n",
        "        rmse = torch.sqrt(torchmetrics.functional.mean_squared_error(predictions, y))\n",
        "        mae = torchmetrics.functional.mean_absolute_error(predictions, y)\n",
        "        accuracy = accuracy_func(predictions, y)\n",
        "        r2 = r2_func(predictions, y)\n",
        "        explained_variance = explained_variance_func(predictions, y)\n",
        "        metrics = {\n",
        "            \"val_loss\": loss,\n",
        "            \"RMSE\": rmse,\n",
        "            \"MAE\": mae,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"R2\": r2,\n",
        "            \"ExplainedVar\": explained_variance,\n",
        "        }\n",
        "        self.log_dict(metrics)\n",
        "        return predictions.reshape(batch[1].size()), y.reshape(batch[1].size())\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        pass\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=self.hparams.learning_rate,\n",
        "            weight_decay=self.hparams.weight_decay,\n",
        "        )\n",
        "    \n",
        "    @staticmethod\n",
        "    def add_task_specific_arguments(parent_parser):\n",
        "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument(\"--learning_rate\", \"--lr\", type=float, default=1e-3)\n",
        "        parser.add_argument(\"--weight_decay\", \"--wd\", type=float, default=1.5e-3)\n",
        "        parser.add_argument(\"--loss\", type=str, default=\"mse\")\n",
        "        return parser"
      ],
      "metadata": {
        "id": "g6JyHdRhMelw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatioTemporalCSVDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        feat_path: str,\n",
        "        adj_path: str,\n",
        "        batch_size: int = 64,\n",
        "        seq_len: int = 12,\n",
        "        pre_len: int = 3,\n",
        "        split_ratio: float = 0.8,\n",
        "        normalize: bool = True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super(SpatioTemporalCSVDataModule, self).__init__()\n",
        "        self._feat_path = feat_path\n",
        "        self._adj_path = adj_path\n",
        "        self.batch_size = batch_size\n",
        "        self.seq_len = seq_len\n",
        "        self.pre_len = pre_len\n",
        "        self.split_ratio = split_ratio\n",
        "        self.normalize = normalize\n",
        "        self._feat = load_features(self._feat_path)\n",
        "        self._feat_max_val = np.max(self._feat)\n",
        "        self._adj = load_adjacency_matrix(self._adj_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_data_specific_arguments(parent_parser):\n",
        "        parser = argparse.ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument(\"--batch_size\", type=int, default=32)\n",
        "        parser.add_argument(\"--seq_len\", type=int, default=12)\n",
        "        parser.add_argument(\"--pre_len\", type=int, default=3)\n",
        "        parser.add_argument(\"--split_ratio\", type=float, default=0.8)\n",
        "        parser.add_argument(\"--normalize\", type=bool, default=True)\n",
        "        return parser\n",
        "\n",
        "    def setup(self, stage: str = None):\n",
        "        (\n",
        "            self.train_dataset,\n",
        "            self.val_dataset,\n",
        "        ) = generate_torch_datasets(\n",
        "            self._feat,\n",
        "            self.seq_len,\n",
        "            self.pre_len,\n",
        "            split_ratio=self.split_ratio,\n",
        "            normalize=self.normalize,\n",
        "        )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        return DataLoader(self.val_dataset, batch_size=len(self.val_dataset))\n",
        "\n",
        "    @property\n",
        "    def feat_max_val(self):\n",
        "        return self._feat_max_val\n",
        "\n",
        "    @property\n",
        "    def adj(self):\n",
        "        return self._adj"
      ],
      "metadata": {
        "id": "_jKq7M_uRdgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATHS = {\n",
        "    \"shenzhen\": {\"feat\": \"data/sz_speed.csv\", \"adj\": \"data/sz_adj.csv\"},\n",
        "    \"losloop\": {\"feat\": \"/content/drive/My Drive/TGCN-Reproduce/data/los_speed.csv\", \"adj\": \"/content/drive/My Drive/TGCN-Reproduce/data/los_adj.csv\"},\n",
        "}\n",
        "\n",
        "\n",
        "def get_model(args, dm):\n",
        "    model = TGCN(adj=dm.adj, hidden_dim=args.hidden_dim)\n",
        "    return model\n",
        "\n",
        "\n",
        "# def get_task(args, model, dm):\n",
        "#     task = getattr(tasks, args.settings.capitalize() + \"ForecastTask\")(\n",
        "#         model=model, feat_max_val=dm.feat_max_val, **vars(args)\n",
        "#     )\n",
        "#     return task\n",
        "\n",
        "\n",
        "# def get_callbacks(args):\n",
        "#     checkpoint_callback = pl.callbacks.ModelCheckpoint(monitor=\"train_loss\")\n",
        "#     plot_validation_predictions_callback = utils.callbacks.PlotValidationPredictionsCallback(monitor=\"train_loss\")\n",
        "#     callbacks = [\n",
        "#         checkpoint_callback,\n",
        "#         plot_validation_predictions_callback,\n",
        "#     ]\n",
        "#     return callbacks\n",
        "\n",
        "\n",
        "def main_supervised(args):\n",
        "    dm = SpatioTemporalCSVDataModule(\n",
        "        feat_path=DATA_PATHS[args.data][\"feat\"], adj_path=DATA_PATHS[args.data][\"adj\"], **vars(args)\n",
        "    )\n",
        "    model = get_model(args, dm)\n",
        "    task = get_task(args, model, dm)\n",
        "    callbacks = get_callbacks(args)\n",
        "    trainer = pl.Trainer.from_argparse_args(args, callbacks=callbacks)\n",
        "    trainer.fit(task, dm)\n",
        "    results = trainer.validate(datamodule=dm)\n",
        "    return results\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    rank_zero_info(vars(args))\n",
        "    results = globals()[\"main_\" + args.settings](args)\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser = pl.Trainer.add_argparse_args(parser)\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--data\", type=str, help=\"The name of the dataset\", choices=(\"shenzhen\", \"losloop\"), default=\"losloop\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        help=\"The name of the model for spatiotemporal prediction\",\n",
        "        choices=(\"GCN\", \"GRU\", \"TGCN\"),\n",
        "        default=\"GCN\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--settings\",\n",
        "        type=str,\n",
        "        help=\"The type of settings, e.g. supervised learning\",\n",
        "        choices=(\"supervised\",),\n",
        "        default=\"supervised\",\n",
        "    )\n",
        "    parser.add_argument(\"--log_path\", type=str, default=None, help=\"Path to the output console log file\")\n",
        "    parser.add_argument(\"--send_email\", \"--email\", action=\"store_true\", help=\"Send email when finished\")\n",
        "\n",
        "    temp_args, _ = parser.parse_known_args()\n",
        "\n",
        "    # parser = getattr(utils.data, temp_args.settings.capitalize() + \"DataModule\").add_data_specific_arguments(parser)\n",
        "    # parser = getattr(models, temp_args.model_name).add_model_specific_arguments(parser)\n",
        "    # parser = getattr(tasks, temp_args.settings.capitalize() + \"ForecastTask\").add_task_specific_arguments(parser)\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "    # utils.logging.format_logger(pl._logger)\n",
        "    # if args.log_path is not None:\n",
        "    #     utils.logging.output_logger_to_file(pl._logger, args.log_path)\n",
        "\n",
        "    # try:\n",
        "    #     results = main(args)\n",
        "    # except:  # noqa: E722\n",
        "    #     traceback.print_exc()\n",
        "    #     if args.send_email:\n",
        "    #         tb = traceback.format_exc()\n",
        "    #         subject = \"[Email Bot][❌] \" + \"-\".join([args.settings, args.model_name, args.data])\n",
        "    #         utils.email.send_email(tb, subject)\n",
        "    #     exit(-1)\n",
        "\n",
        "    # if args.send_email:\n",
        "    #     subject = \"[Email Bot][✅] \" + \"-\".join([args.settings, args.model_name, args.data])\n",
        "    #     utils.email.send_experiment_results_email(args, results, subject=subject)"
      ],
      "metadata": {
        "id": "Sqj9zIbeR5_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "0h6wHY9wpstH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "qJmI2L1ipx6L"
      }
    }
  ]
}